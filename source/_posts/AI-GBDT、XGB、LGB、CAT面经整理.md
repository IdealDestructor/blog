---
title: GBDT、XGB、LGB、CAT面经整理
date: 2021-11-18 14:22:35
tags: [机器学习]
categories: 人工智能
widgets: null
password: 328
abstract: 这是一篇加密博文，请输入密码后查看
message: 这里需要密码才能访问。
wrong_pass_message: 抱歉, 这个密码看着不太对, 请再试试.
---

注意，下文的原始的 gbdt 是以 sklearn 中的 gbdt 的实现为例子来阐述的，因为 gbdt 的改进版本有很多，为了叙述方便，使用这个最为人所知的实现来描述。

<!--more-->

*   **你有自己用过别的模型然后调参之类的吗？能说一下基本的调参流程吗？XGB 知道吗，以 XGB 为例子说一下调参流程吧**

（个人的思路）：一般来说采用贝叶斯优化或者遗传算法等启发式的优化算法确定相对最佳参数（如果不熟悉的话用随机搜索也是可以的，或者网格搜索但是参数得到步长设置的很大，一步一步确定相对最优参数的区间），然后再根据实际的模型在验证集上的表现做一些微调，对于过拟合优先调整 max_depth 和树的数量，在实际使用过程中这两个参数对于模型的整体效果影响很大很明显。对于欠拟合，反着来就行了。

*   **XGB 和 GBDT 的区别有哪些？**

**1、算法层面：**

（1）损失函数的二阶泰勒展开；（具体的二阶泰勒展开的阐述下面那点会详细描述）

（2）树的正则化概念的引入，对叶节点数量和叶子节点输出进行了约束，方式是将二者形成的约束项加入损失函数中；

（3）二阶泰勒展开与树正则化推出了新的叶子节点输出的计算公式而不是原始 gbdt 那样的简单平均；

（4）a、对于基础学习器的改进，

![](https://pic4.zhimg.com/80/v2-25eafe70632d26e6e106c1cc86a46d5f_1440w.png)

分裂的时候自动根据是否产生正增益指导是否进行分裂，因为引入了正则项的概念，分裂的时候这个预剪枝更加严苛；

b、对于缺失值的处理，xgboost 根据左右子节点的增益大小将缺失值分到增益大的节点中，而 sklearn 中的 gbdt 是无法处理缺失值的，因为 sklearn 中的 gbdt 是以 sklearn 中的 cart 为基学习器的，而 sklearn 中的 cart 也并没有实现对缺失值的处理功能。

（5）学习率，Shrinkage，对每一颗树都乘以小于 1 的学习率，来削弱每一颗树的影响，这样的结果就是会引入更多的树来处理使得基学习器得数量变多，从而降低过拟合，不过其实 sklearn 中的 gbdt 也实现了。。。不知道为什么这么多人把这一点也列为不同；

（6）、引入了随机森林使用的列采样功能，便于降低过拟合；

（7）、引入了许多近似直方图之类的优化算法来进一步提高树的训练速度与抗过拟合的能力，这个比较复杂，因为实现了很多种算法，后面单独写一篇来总结；

**2、工程层面**


(1)、对每个特征进行分块（block）并排序（pre_sort），将排序后的结构保存在内存中，这样后续分裂的时候就不需要重复对特征进行排序然后计算最佳分裂点了，并且能够进行并行化计算. 这个结构加速了 split finding 的过程，只需要在建树前排序一次，后面节点分裂时直接根据索引得到梯度信息。

（2）

[金贵涛：对 xgboost 的理解](https://zhuanlan.zhihu.com/p/75217528)

其它更复杂的工程优化处理见这里。。。。

**为什么 xgb 用二阶导：**

**1、形式上的统一：**

**下面来自 xgb 的官网叙述：**

![](https://pic1.zhimg.com/v2-aaead4a8a201fa97be58f18506824970_r.jpg)

可以看到，损失函数为 mse 的时候，注意，此时我们没有进行二阶泰勒展开

![](https://pic2.zhimg.com/v2-84f49f1a236d3df1043e260ca3ac1029_r.jpg)

对比可以看到，其它损失函数泰勒展开之后去掉常数最终的形式和 mse 的不泰勒展开的形式是完全一致的（mse 的二阶梯为常数 1，一阶梯度是 y_pred-y_True）, 这么做的好处是，这样的话，1、 xgboost 在对 mse 的损失函数设计完求解器之后，这一套代码可以直接复用给别的损失函数来使用，因为我们如果不做二阶泰勒展开的话，比如新的损失函数是二元交叉熵，在工程设计上，我们还要将损失函数的求导，然后把求导之后的式子写出来：

![](https://pic3.zhimg.com/80/v2-07c781f80bea7d05e63a3e9219948216_1440w.png)

设计一个新的求解器去求解，很麻烦。

而进行了这样的设计之后，后续如果还有一些什么别的损失函数，底层的求解 mse 的代码可以直接使用，使用者只需要自行去求解新的损失函数的一阶梯度和二阶梯度的表达式，然后通过 xgboost 的自定义损失函数的功能就可以实现使用完备的 xgboost 的框架来求解自己的损失函数的最优值了。

2、关于速度的问题，gbdt 的前向分布的求解思路可以说就和我们常见的逻辑回归求解的梯度下降是类似的，线性回归的梯度下降每一轮通过更新参数的方式接近损失函数的最优值，而 gbdt 则是用基学习器去拟合，相对而言，xgboost 类似于使用牛顿法来求解线性回归，所以下面从牛顿和梯度下降的角度来阐述，的实际上我们常说的牛顿法比梯度下降法快是不准确的，应该是牛顿法的收敛速度要比梯度下降法快，也就是说牛顿法使用的迭代次数相对于梯度下降法要更少，但是由于涉及到计算二阶导的信息，牛顿法不一定在算法训练的时间上总比梯度下降法快，只是相对于梯度下降法而言，更少的迭代达到最优，这一点来看，并不算是优势。

**没明白为什么有人说 xgboost 的二阶泰勒展开要更快？？？希望有大佬来解释一下这个问题。**

*   **xgb 怎么梯度下降的**：

和 gbdt 是一样的，t-1 轮的所有的子数的总预测值和真实值进入损失函数的负梯度的表达式计算得到负梯度作为第 t 轮要拟合的标签值。严格来说，这是前向分布算法，虽然他和梯度下降法的思路非常相似，但是梯度下降法对于每一轮的负梯度的使用方法是作为上一轮参数的参数的更新量，而 xgb 是直接将其作为标签值用新的基学习器去拟合。

*   **xgb 的正则化**

![](https://pic3.zhimg.com/80/v2-352072b3acc56207f3fd2bdd6ca7ecba_1440w.png)

叶子节点个数的正则化约束，参数为 gamma，

叶子节点输出值的正则化约束，参数是 lambda。

**XGB 特征重要性程度是怎么判断的？**

xgb 的特征重要性的方式衍生的非常花俏了。

![](https://pic1.zhimg.com/v2-718d7431859a643f6068ee1b6cddb1e4_r.jpg)

官网上给出的方案，total_gain 就是特征带来的总的分裂增益，也就是我们常规意义上的分裂总增益，weight，被用来作为分裂节点的次数，也就是我们常规意义上的分裂总次数，gain=total_gain/weight，计算的是每一次分裂带来的平均增益，total_cover 表示特征分裂的样本数，举个例子，假设初始样本有 10000 个，第一次分裂的时候使用了特征 A，也就是特征 A 在这 10000 个样本上分裂，则此时的 cover 值为 10000，假设根据特征 A 分裂出左枝的样本有 1000 个，右边有 9000 个，而在左枝特征 B 是最优特征根据这 1000 个样本进行分裂，则 B 当前的 cover 是 1000，依次类推最后求和。

而 cover 显然就是 total_cover/weight，也就是平均每次分裂所 “负责” 的样本数。

**XGB 很容易理解它的回归和二分类，如何理解多分类呢？**

[https://www.cnblogs.com/always-fight/p/9400346.html](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/always-fight/p/9400346.html)

[GBDT 用于分类问题 - 1 直在路上 1 - 博客园](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/always-fight/p/9400346.html)

[GBDT 用于分类问题 - 1 直在路上 1 - 博客园](https://link.zhihu.com/?target=https%3A//www.cnblogs.com/always-fight/p/9400346.html)

思路就是 ovr，比如三分类，每一轮是生成 3 颗数，不过损失函数还是用的多分类的损失函数比如常见的 logloss，具体的可以见上面这篇文章写的很清楚了。

**XGB 和 LGB 区别：**

1、直方图优化，对连续特征进行分桶，在损失了一定精度的情况下大大提升了运行速度，并且在 gbm 的框架下，基学习器的 “不精确” 分箱反而增强了整体的泛化性能；

2、goss 树的引入；

3、efb，对稀疏特征做了 “捆绑” 的优化功能；

4、直接支持对于类别特征进行训练（实际上内部是对类别特征做了类似编码的操作了）

5、树的生长方式由 level-wise 变成 leaf-wise；

算法层面的优化具体可见：

[马东什么：不手写 lightgbm（1）—怎么分桶的](https://zhuanlan.zhihu.com/p/85053333)[马东什么：不手写 lightgbm（2）—其它的一些特性](https://zhuanlan.zhihu.com/p/85312276)

工程层面的优化可见 lightgbm 原作者之一的知乎回答：

[如何看待微软新开源的 LightGBM?](https://www.zhihu.com/question/51644470/answer/130946285)

**xgboost 与 RF 的对比**

**GBDT 与 RF 区别**

1、组成随机森林的树可以是分类树，也可以是回归树；而 GBDT 只由回归树组成，GBDT 的会累加所有树的结果，而这种累加是无法通过分类完成的，因此 GBDT 的树都是 CART 回归树，而不是分类树（尽管 GBDT 调整后也可以用于分类但不代表 GBDT 的树为分类树）

2、组成随机森林的树可以并行生成；而 GBDT 只能是串行生成

3、对于最终的输出结果而言，随机森林采用多数投票或简单平均等；而 GBDT 则是将所有结果累加起来，或者加权累加起来（存在学习率）

4、随机森林对异常值不敏感，GBDT 对异常值非常敏感

5、随机森林对训练集一视同仁，GBDT 是基于权值的弱分类器的集成

6、随机森林是通过减少模型方差提高性能，GBDT 是通过减少模型偏差提高性能，但是 xgb 引入了正则项和列采样等等正则化手段之后，可以在少量增加偏差的情况下大幅度缩减模型的方差。

**xgb 的预排序算法是怎么做的呢？**

将原始特征进行排序之后以块的形式保存到内存中，在块里面保存排序后的特征值及对应样本的引用，以便于获取样本的一阶、二阶导数值，但意味着除了保存原始特征之外还要保存原始特征的排序结果，耗内存。

**rf 和 xgb 哪个对异常点更敏感**

xgb 明显敏感的多，当然对 rf 也是有一定影响的，rf 的每棵数的生成是独立的，异常点数量不多的情况下异常点常常和正常样本中的某些样本合并在一个分支里。

但是 xgb 不一样，异常样本的 t-1 轮的预测值和真实标签计算出来的负梯度会一直很大，假设当到达某一轮的时候，所有正常样本的计算得到的负梯度都很小而异常样本的负梯度很大例如【0.0000001,0.0000001,0.0000001,0.0000001,0.0000001,10】, 这个时候新树会可能会继续进行不正常的分裂为 [0.0000001,0.0000001,0.0000001,0.0000001,0.0000001],[10]，而这样的分裂是不合理的，因为异常值本身可能是因为某些人为失误导致的数据记录错误，或者异常样本完全是属于另外一种分布，此时强制要进行模型训练会导致模型的结果有偏从而发生过拟合。

当然异常样本数量很少比如 10 个以内的时候而正常样本有 100000000 个其实基本没什么影响，但是如果占比较高的话是会产生影响的。

**xgb 何时停止分裂？**

1、人工设定的参数，max_depth,min_data_in_leaf 等等，这类通过超参数形式限制树的复杂度的方法都会引发 xgb 的分裂的停止，也就是常说的预剪枝；

2、人工不限制，自由生长的情况下，当分裂增益小于 0 则基学习器停止分裂

**XGB 怎么解决过拟合？怎么剪枝？怎么选择特征？**

叶节点个数和叶节点权重（输出值）树的正则项，各种预剪枝的超参数（最大深度、最大叶节点个数、最小分裂增益、学习率、早停等等）控制树结构的复杂度，行列采样的引入、以及各类近似算法等。每轮计算所有特征各自的最佳分裂点，比较所有特征按照最佳分裂点分裂之后的分裂增益大小，使用分裂增益最大的特征在其最佳分裂点分裂一次，然后继续循环。。。。。

**对比一下 XGB 和 lightGBM 在节点分裂时候的区别**

xgb 是 level-wise，lgb 是 leaf-wise，level-wise 指在树分裂的过程中，同一层的非叶子节点，只要继续分裂能够产生正的增益就继续分裂下去，而 leaf-wise 更苛刻一点，同一层的非叶子节点，仅仅选择分裂增益最大的叶子节点进行分裂。

**Lgb 相对于 xgb 的优缺点**

优点：直方图算法—更高（效率）更快（速度）更低（内存占用）更泛化（分箱与之后的不精确分割也起到了一定防止过拟合的作用）；

缺点：直方图较为粗糙，会损失一定精度，但是在 gbm 的框架下，基学习器的精度损失可以通过引入更多的 tree 来弥补。

**Xgb 叶子节点怎么计算值的**

损失函数引入树正则化概念并二阶泰勒展开，去掉常数项之后得到最终的 xgb 损失函数的形式：，

![](https://pic4.zhimg.com/80/v2-177ff6a31a37d839ae77a6e8fafa5dff_1440w.jpg)

然后根据此损失函数推导得到叶节点的计算公式

![](https://pic4.zhimg.com/80/v2-7a596955e278896788d78619a0ae6113_1440w.png)

**xgb 与 lr 相比优缺点在哪里？**

这类问题是一个很广的问题，涉及到不同算法的适用性，这个后面单独总结一下吧

**xgb 中 l1 正则怎么用的**

xgb 中 l1 表示对叶节点个数的约束项的系数，而 l2 则是叶子节点权重的约束项系数。

**xgboost 对特征缺失敏感吗，对缺失值做了什么操作，存在什么问题**

不敏感，可以自动处理，处理方式是将 missing 值分别加入左节点 右节点取分裂增益最大的节点将 missing 样本分裂进这个节点 。这种处理方式的问题在 xgboost 仅仅在特征的非缺失的值上进行分裂然后 missing 值直接放入其中一个节点，显然当缺失值很多的情况下，比如缺失 80%，那么 xgb 分裂的时候仅仅在 20% 的特征值上分裂，这是非常容易过拟合的。

**xgb 和 lgb 在特征、数据并行上存在什么差异？**

1）特征并行

lgbm 特征并行的前提是每个 worker 留有一份完整的数据集，但是每个 worker 仅在特征子集上进行最佳切分点的寻找；worker 之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个 worker 进行切分即可。

xgb 的特征并行与 lgbm 的最大不同在于 xgb 每个 worker 节点中仅有部分的列数据，也就是垂直切分，每个 worker 寻找局部最佳切分点，worker 之间相互通信，然后在具有最佳切分点的 worker 上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他 worker 才能开始分裂。

二者的区别就导致了 lgbm 中 worker 间通信成本明显降低，只需通信一个特征分裂点即可，而 xgb 中要广播样本索引。

2）数据并行

当数据量很大，特征相对较少时，可采用数据并行策略。

lgbm 中先对数据水平切分，每个 worker 上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得 worker 间的通信成本降低一倍，因为只用通信以此样本量少的节点。

xgb 中的数据并行也是水平切分，然后单个 worker 建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个 worker 上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个 worker 间的通信量也就变得很大。

3）投票并行（lgbm）

当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。

大致思想是：每个 worker 首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择 top 的特征进行直方图的合并，再寻求全局的最优分割点。这个方法我没有找到很好的解释，因此，面试过程中答出前面两种我觉得就 ok 了吧。

**为什么 xgboost 不用后剪枝**

后剪枝计算代价太高了，合并一次叶节点就要计算一次测试集的表现，数据量大的情况下非常消耗时间，而且也并不是特别必要，因为这样很容易过拟合测试集。

**GBDT 和 RF 哪个树比较深**

第一种解释： RF 深。随机森林的思路是用大量低偏差高方差的基学习器进行集成，简单平均（不过 lightgbm 中的 rf 貌似不太一样，没有细致研究过），降低方差，所以希望每一个基学习器的精度尽量高，如果随机森林的基学习器偏差大，对于 100 个或者 10000 个精度为 0.6 的学习器，很难通过随机森林的集成方式来达到好的效果；而 gbdt 本身就是对误差的不断拟合，本身就是一个偏差很低的集成框架，那么为了同时也使得方差缩小，需要基学习器的泛化性能好一些，避免整个框架的偏差很低但方差很大的窘境；

第二种解释：随机森林每一颗树都是独立的，每一颗树都是以原始标签进行训练的，在不进行任何限制的情况下会生长的比较深，而 gbdt 不一样，每一轮都是以上一轮的负梯度为新标签进行训练，训练到一定程度的时候我们去观察负梯度就可以发现，因为很多样本已经得到很好的拟合，所以负梯度会比较小，比如可能是这样的 [0.000000001,0.000000001,0.000000001,0.0000000015......]，这样树在分裂的时候实际上再进行分裂的增益并不大，甚至分裂之后的增益反而减少，这就导致了基树训练的时候很早就停止了，从而导致树的深度降低。

**为什么 gbdt 不能用分类树？**

分类树无法处理连续值，负梯度一般都是连续值。

**lightGBM 直方图加速原理？ lightGBM 处理类别变量的原理？ lightGBM 在特征分裂时相比 GBDT 和 xgboost 有什么优化？ lightGBM 特征分裂加速的原理是什么？**

[马东什么：不手写 lightgbm（1）—怎么分桶的](https://zhuanlan.zhihu.com/p/85053333)[马东什么：不手写 lightgbm（2）—其它的一些特性](https://zhuanlan.zhihu.com/p/85312276)[如何看待微软新开源的 LightGBM?](https://www.zhihu.com/question/51644470/answer/130946285)

算法和工程方面的都在这里了。

 **lightGBM 重要性怎么评估？**

和 xgb 一样。不过 lgb 中没有 cover 这一评价方式。

**lightGBM 节点怎么分裂的？**

**这个问题，去 github 上看了问答才搞清楚，**

[马东什么：不手写 lightgbm（1）—怎么分桶的](https://zhuanlan.zhihu.com/p/85053333)[What is the LightGBM splitting criterion? · Issue #1774 · microsoft/LightGBM](https://link.zhihu.com/?target=https%3A//github.com/microsoft/LightGBM/issues/1774)

已经有人在 github 上提问了，按照开发者的意思，和 xgboost 是一样的。

![](https://pic1.zhimg.com/v2-f1a02b01005deda954d0cfc763e5184c_r.jpg)

**用的 xgboost 也许要对特征进行标准化吗？**

不用做标准化，但是要对分布不稳定的特征进行处理，比如分箱，log 变换之类的，因为我们交叉验证的时候如果特征分布特别不规则，可能每一折的训练集和开发集上划分到的特征差异性比较大，这是包括 gbdt 算法之外所有算法都 需要进行处理的，机器学习的基本假设是同分布，如果不同数据集的特征分布差异性太大，这样的特征没法直接使用必须要转换一下。

**XGBoost 或 GBDT 中怎么控制模型复杂度？XGBoost 的参数选择（怎么调参）。。。**

自己看参数吧

**xgboost 怎么处理高维稀疏数据？**

xgboost 原理中的稀疏感知是关于缺失值的，

![](https://pic4.zhimg.com/v2-20c2c3652bb887d7c1e4b5c3af81f537_r.jpg)

就是在非缺失的样本上做分裂然后缺失值根据分别进入左右节点带来的增益来决定要划分到哪个节点。如果是常规意义上的高基数类别特征进行 onehot 之后的 0-1 矩阵，xgb 没有什么特别的处理方案。

**数据标签值都在 0-1000 范围之内，随机森林和 gbdt 做回归的值能否超过 1000.**

![](https://pic1.zhimg.com/80/v2-cda2722e0c9a1a44521ca51332a1ad80_1440w.png)

以原始的 gbdt 的叶子节点输出为例，假设学习率为 1，以 mse 为损失函数的情况下上面的式子可以变成：

![](https://www.zhihu.com/equation?tex=w_%7Bj%7D%3D%5Cfrac%7B%5Csum_%7Bi%5Cin+I_%7Bj%7D%7D%5E%7B%7D%7Bypred-ytrue%7D%7D%7Bn%7D) 其中 n 为这个叶子节点的样本数量。。。。。

这个问题。。。我还真没想到什么答案。。。。

**GBDT 为什么用 CART 回归树做基学习器？**

[GBDT 算法原理深入解析](https://link.zhihu.com/?target=http%3A//xudongyang.coding.me/gbdt/)

这一篇说的很清晰。

> 基于梯度提升算法的学习器叫做 GBM(Gradient Boosting Machine)。理论上，GBM 可以选择各种不同的学习算法作为基学习器。现实中，用得最多的基学习器是决策树。为什么梯度提升方法倾向于选择决策树（通常是 CART 树）作为基学习器呢？这与决策树算法自身的优点有很大的关系。决策树可以认为是 if-then 规则的集合，易于理解，可解释性强，预测速度快。同时，决策树算法相比于其他的算法需要更少的特征工程，比如可以不用做特征标准化，可以很好的处理字段缺失的数据，也可以不用关心特征间是否相互依赖等。决策树能够自动组合多个特征，它可以毫无压力地处理特征间的交互关系并且是非参数化的，因此你不必担心异常值或者数据是否线性可分（举个例子，决策树能轻松处理好类别 A 在某个特征维度 x 的末端，类别 B 在中间，然后类别 A 又出现在特征维度 x 前端的情况）。不过，单独使用决策树算法时，有容易过拟合缺点。所幸的是，通过各种方法，抑制决策树的复杂性，降低单颗决策树的拟合能力，再通过梯度提升的方法集成多个决策树，最终能够很好的解决过拟合的问题。由此可见，梯度提升方法和决策树学习算法可以互相取长补短，是一对完美的搭档。至于抑制单颗决策树的复杂度的方法有很多，比如限制树的最大深度、限制叶子节点的最少样本数量、限制节点分裂时的最少样本数量、吸收 bagging 的思想对训练样本采样（subsample），在学习单颗决策树时只使用一部分训练样本、借鉴随机森林的思路在学习单颗决策树时只采样一部分特征、在目标函数中添加正则项惩罚复杂的树结构等。

**GBDT 不擅长处理离散特征，你在应用的时候是怎么处理的**

GBDT 不擅长处理高基数类别特征，如果基数低，问题也不大。原因可见：

[马东什么：为何常规的 gbdt 和决策树不适用于高基数特征的场景](https://zhuanlan.zhihu.com/p/85353086)

lightgbm 和 catboost 内部对类别特征有自己的特征工程方案，而 xgboost 这类无法直接处理类别特征的库则一般来说：1、单值离散用编码的方法；2、多值离散用 embedding

**GBDT 在回归和多分类当中有什么不同，在预测的时候的流程是怎样的**

损失函数不同（废话），预测存在差异。

回归则直接加权求和输出全部基学习器的预测结果，分类还要将预测结果加权求和的结果放到 sigmoid 或者 softmax 里面转化为概率值。

**gbdt 如果损失函数换成 exponent 会怎样，有什么变化？**

变成 adaboost。

具体可见：

**（boost 框架的历史回顾中，有空补上）**

**GBDT 在什么情况下比逻辑回归算法要差？**

高维稀疏的数据集，gbdt 对维度超高的稀疏数据集，其正则项约束基本没用，并且决策空间会变成太多零散的决策小空间，具体可见上 gbdt 为何不好处理高基数类别特征的问题。

而 lr 的 l1 正则项可以很好的约束没啥用 的稀疏特征，直接 w 置 0 即可。

**GBDT 对输入数据有什么要求，如果效果比较差，可能是什么原因造成的？**

如果训练集的效果很差，说明原始数据相对于 gbdt 算法来说实在太差了，特征基本没什么区分度，xgb 这种拟合能力超强的算法都无法很好的拟合；

如果训练集的效果很好测试集很差，并且二者的差距非常大（比如 10 个点以上），考虑特征分布的问题，应该是有一些强特的分布在训练集和测试集上差异太大了。

如果训练集效果很好，测试集稍差一点，二者差异并不是很大，考虑调参。

**xgb 和 lgb 的并行实现的比较**

1）特征并行

lgbm 特征并行的前提是每个 worker 留有一份完整的数据集，但是每个 worker 仅在特征子集上进行最佳切分点的寻找；worker 之间需要相互通信，通过比对损失来确定最佳切分点；然后将这个最佳切分点的位置进行全局广播，每个 worker 进行切分即可。

**简单说就是 lgb 的每一个 worker 处理一个特征，按列来拆分到多个 worker 上进行计算的**

xgb 的特征并行与 lgbm 的最大不同在于 xgb 每个 worker 节点中仅有部分的列数据，也就是垂直切分，每个 worker 寻找局部最佳切分点，worker 之间相互通信，然后在具有最佳切分点的 worker 上进行节点分裂，再由这个节点广播一下被切分到左右节点的样本索引号，其他 worker 才能开始分裂。

**简单说就是 xgb 的每一个 worker 处理所有特征，但是每个 worker 只处理一部分取值范围里的最佳切分点，比如 5 个 worker 对一个特征的取值范围切分为 5 份，在每一份上计算最佳切分点，然后 5 份上的 5 个最佳切分点做比较取最好**

**一个是竖着切分，一个是横着切分。**

二者的区别就导致了 lgbm 中 worker 间通信成本明显降低，只需通信一个特征分裂点即可，而 xgb 中要广播样本索引。

2）数据并行

当数据量很大，特征相对较少时，可采用数据并行策略。

lgbm 中先对数据水平切分，每个 worker 上的数据先建立起局部的直方图，然后合并成全局的直方图，采用直方图相减的方式，先计算样本量少的节点的样本索引，然后直接相减得到另一子节点的样本索引，这个直方图算法使得 worker 间的通信成本降低一倍，因为只用通信以此样本量少的节点。

xgb 中的数据并行也是水平切分，然后单个 worker 建立局部直方图，再合并为全局，不同在于根据全局直方图进行各个 worker 上的节点分裂时会单独计算子节点的样本索引，因此效率贼慢，每个 worker 间的通信量也就变得很大。

**3）投票并行（lgbm）**

当数据量和维度都很大时，选用投票并行，该方法是数据并行的一个改进。数据并行中的合并直方图的代价相对较大，尤其是当特征维度很大时。

大致思想是：每个 worker 首先会找到本地的一些优秀的特征，然后进行全局投票，根据投票结果，选择 top 的特征进行直方图的合并，再寻求全局的最优分割点。这个方法。。。不懂，上述都是搬运来的。。

**xgboost 有哪些参数？**

- 学习率 eta ：学习率越小，迭代次数越多。

- 最小孩子权重 min-child-weight：控制叶子结点中二阶导数和的最小值，即样本的数量越少（由于 h 大约均在 0.01 附近），越容易过拟合

- 最大深度 max_depth  
- 最大叶子结点数 max_leaf_node

- 后剪枝参数 gamma  
  -L2 参数 lambda  
  -L1 参数 alpha (控制模型复杂度)

- 样本随机采样 subsample；列采样比例 colsample_bytree  
  (5)xgboost 有哪些优点？  
- 树节点分裂方法，利用近似算法，二阶导数为权重值的分位数作为切分点  
- 自动学习特征缺失值的方向  
- 列抽样 (借鉴随机森林)，行抽样  
- 学习率 (eta) 的 shrinkage，增加迭代次数  
- 自定义损失函数  
- 特征预排序  
  (6)xgboost 和 gbdt 的区别？  
  1）GBDT 是以 CART 作为基分类器，xgboost 支持线性分类器，其中线性分类器的 xgboost 相当于正则化的逻辑回归（分类问题）或  
  线性回归（回归问题）  
  2）GBDT 的目标函数含有一阶信息，xgboost 的目标函数含有二阶信息，最小化目标函数可得关于函数空间 f(t) 的梯度迭代或牛顿迭代，  
  牛顿法能更快的收敛。同时 xgboost 加入了正则项，控制了模型的复杂度。  
  (7) Lightgbm 对 xgboost 有哪些改进？  
  -Histgram 算法 将浮点型数值离散为 K 个，统计离散值的累积量，遍历直方图找最优特征分裂点  
- 直方图加速：叶子结点的直方图可由父亲结点的直方图与兄弟结点的直方图做差得到  
  -leave wise 选取信息增益最大的叶子结点继续分裂（容易过拟合，利用 max_depth 参数控制)

14. xgboost 里面的 lambdarank 的损失函数是什么？

15. xgboost 在什么地方做的剪枝，怎么做的？ 分裂，预剪枝；参数，预剪枝。

16. xgboost 如何分布式？特征分布式和数据分布式？ 各有什么存在的问题？



